

#%%
#í† í°í™”(Tokenization)
from transformers import BertTokenizer
#tok = BertTokenizer.from_pretrained("bert-base-uncased")
'''
í† í°í™” ê³¼ì •ì—ì„œ NotImplementedError: Cannot ì—ëŸ¬ ë°œìƒ
ë²„ì „ ë¬¸ì œ?
'''

tok = BertTokenizer
tok("Hello, how are you doing?")['inputs_ids']
tok("The Frenchman spoke in the [MASK] language and ate ğŸ¥–")['input_ids']
tok("[CLS] [SEP] [MASK] [UNK]")['input_ids']

#í† í¬ë‚˜ì´ì €ëŠ” ìë™ìœ¼ë¡œ ì¸ì½”ë”©ì˜ ì‹œì‘ ([CLS]=101[CLS] = 101[CLS]=101)ë° ì¸ì½”ë”©ì˜ ì¢…ë£Œ(ì¦‰, ë¶„ë¦¬) ([SEP]=102[SEP] = 102[SEP]=102)ì— ëŒ€í•œ í† í°ì„ í¬í•¨
#ë§ˆìŠ¤í‚¹ ([MASK]=103[MASK] = 103[MASK]=103) ë° ì•Œ ìˆ˜ ì—†ëŠ” ê¸°í˜¸ ([UNK]=100) 




#%%
#ì„ë² ë”©(Embeddings)
#ì„ë² ë”©ì„ í†µí•´ ë²¡í„°ë¡œ ë³€í™˜->ì •ê·œí™”
import math
import torch
from torch import nn
class Embed(nn.Module):
    def __init__(self, vocab: int, d_model: int = 512):
        super(Embed, self).__init__()
        self.d_model = d_model
        self.vocab = vocab
        self.emb = nn.Embedding(self.vocab, self.d_model)
        #ì–´íœ˜ì˜ í¬ê¸° ë° ëª¨ë¸ì˜ ì°¨ì› ì§€ì •
        #torch.nn.Embedding(num_embeddings, embedding_dim)
        #Input: IntTensor or LongTensor 
        
        self.scaling = math.sqrt(self.d_model) #ì •ê·œí™”

    def forward(self, x):
        return self.emb(x) * self.scaling


#%%    
#í¬ì§€ì…”ë„ ì¸ì½”ë”©(Positional Encoding)
'''
TransformerëŠ” RNNì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ë¬¸ì¥ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë”°ë¡œ ì œê³µí•´ì¤„ í•„ìš”
ì¸ì½”ë”ì™€ ë””ì½”ë”ì— ëŒ€í•œ ì…ë ¥ ì„ë² ë”©ì— ì¸ì½”ë”©ì„ ì¶”ê°€->ì„ë² ë“œëœ í† í°ì˜ ìƒëŒ€ ìœ„ì¹˜ ì…ë ¥
í¬ì§€ì…”ë‹ ì¸ì½”ë”©ì€ ìˆ«ì ì˜¤ë²„ í”Œë¡œìš° (numerical overflow) ë°©ì§€ë¥¼ ìœ„í•´ ë¡œê·¸ ê³µê°„(log space)ì—ì„œ ì—°ì‚°
'''
import torch
from torch import nn
from torch.autograd import Variable

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int = 512, dropout: float = .1, max_len: int = 5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        
        # Compute the positional encodings in log space
        #ë¹„ì–´ìˆëŠ” tensor ìƒì„±
        pe = torch.zeros(max_len, d_model)
        #position(0-max_len) ìƒì„± (row ë°”ì–‘ -> unsqueeze(dim=1))
        position = torch.arange(0, max_len).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.Tensor([10000.0])) / d_model))
        
        # (iê°€ ì§ìˆ˜ì¼ë•Œ : sin, í™€ìˆ˜ì¼ë•Œ : cos)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # [max_len, hid_dim] -> [1, max_len, hid_dim]
        pe = pe.unsqueeze(0)
        # ë§¤ê°œ ë³€ìˆ˜ë¡œ ê°„ì£¼ë˜ì§€ ì•ŠëŠ” ë²„í¼ë¥¼ ë“±ë¡í•˜ëŠ” ë° ì‚¬ìš©
        #optimizerê°€ updateí•˜ì§€ ì•ŠëŠ”ë‹¤. / state_dictì—ëŠ” ì €ì¥ëœë‹¤. / GPUì—ì„œ ì‘ë™í•œë‹¤.
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)
        #pe[:, :out1.shape[1]] : xì˜ shapeì— ë§ì¶œ ìˆ˜ ìˆë„ë¡ indexing
        return self.dropout(x)



#%%
#ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜(Multi-Head Attention)
'''
Attentionì€ "ì£¼ì–´ì§„ ì¿¼ë¦¬(Query)ì— ëŒ€í•´ì„œ ëª¨ë“  í‚¤(Key)ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê°ê° êµ¬í•´ í‚¤ì™€ ë§¤í•‘ë˜ì–´ ìˆëŠ” ê°’(Value)ì— ë°˜ì˜" 
ì–´í…ì…˜ ë ˆì´ì–´(attention layer)ëŠ” ì¿¼ë¦¬(Q)ì™€ í‚¤(K), ê°’(V)ìŒ ê°„ì˜ ë§µí•‘ì„ í•™ìŠµ
ì¿¼ë¦¬(query)ëŠ” ì…ë ¥ì˜ ì„ë² ë”©ì´ë©°, ê°’(value)ê³¼ í‚¤(key)ëŠ” íƒ€ê¹ƒ ex)ìœ íŠœë¸Œê²€ìƒ‰ì°½:query, ë™ì˜ìƒ:value
'''
import torch
from torch import nn
class Attention:
    def __init__(self, dropout: float = 0.):
        super(Attention, self).__init__()
        self.dropout = nn.Dropout(dropout)
        self.softmax = nn.Softmax(dim=-1)

    # Q, K, Vë¥¼ ëª¨ë‘ ì…ë ¥ë¬¸ì¥ìœ¼ë¡œë¶€í„° ê°€ì ¸ì˜¨ë‹¤
    def forward(self, query, key, value, mask=None):
        d_k = query.size(-1)
        
        #scores : scale dot product attention
        #query, keyë¥¼ í–‰ë ¬ê³±
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
        # Maskê°€ ìˆë‹¤ë©´ ë§ˆìŠ¤í‚¹ëœ ë¶€ìœ„ -1e9ìœ¼ë¡œ ì±„ìš°ê¸°
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
            #masked_fill : ë§ˆìŠ¤í¬ì—ì„œ íŠ¹ì •ê°’ì„ ë°”ê¿”ì¤€ë‹¤
        p_attn = self.dropout(self.softmax(scores))
        return torch.matmul(p_attn, value)
    
    def __call__(self, query, key, value, mask=None):
        #nn.Moduleì„ ìƒì†ë°›ì€ classì—ì„œ forwardì™€ ë™ì¼í•œ íš¨ê³¼ë¥¼ ë‚¸ë‹¤
        return self.forward(query, key, value, mask)
'''
#ë‹¨ì¼ ì–´í…ì…˜ ë ˆì´ì–´ëŠ” í•˜ë‚˜ì˜ í‘œí˜„ë§Œì„ í—ˆìš©->íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ë‹¤ì¤‘ ì–´í…ì…˜ í—¤ë“œ(multiple attention heads)ê°€ ì‚¬ìš©

ë…¼ë¬¸ì—ì„œëŠ” ê²°í•©ëœ(concatenated) h=8h = 8h=8 ì–´í…ì…˜ ë ˆì´ì–´ë¥¼ ì‚¬ìš©
MultiHead(Q,K,V)=Concat(head1,...,headn)WO
where headi=Attention(QWiQ,KWiK,VWiV)head i = Attention(QWi_Q, KWi_K, VWi_V)
'''


from torch import nn
from copy import deepcopy
class MultiHeadAttention(nn.Module):
    def __init__(self, h: int = 8, d_model: int = 512, dropout: float = 0.1):
        super(MultiHeadAttention, self).__init__()
        self.d_k = d_model // h
        self.h = h
        self.attn = Attention(dropout)
        self.lindim = (d_model, d_model)
        self.linears = nn.ModuleList([deepcopy(nn.Linear(*self.lindim)) for _ in range(4)])
        self.final_linear = nn.Linear(*self.lindim, bias=False)
        self.dropout = nn.Dropout(p=dropout)
        
    def forward(self, query, key, value, mask=None):
        if mask is not None:
            mask = mask.unsqueeze(1)
        
        #Q,K,Vë¥¼  d_k, d_k, d_v ì°¨ì›ìœ¼ë¡œ projection -> Q,K,Vë¥¼ head ìˆ˜ ë§Œí¼ ë¶„ë¦¬
        #   -> (batch_size, head, seq_len, head_dim)
        query, key, value = [l(x).view(query.size(0), -1, self.h, self.d_k).transpose(1, 2) \
                             for l, x in zip(self.linears, (query, key, value))]
        
        '''
        q = nn.Linear(hid_dim, hid_dim)(q)
        k = nn.Linear(hid_dim, hid_dim)(k)
        v = nn.Linear(hid_dim, hid_dim)(v)
        # ì—¬ê¸°ê¹Œì§€ëŠ” ìœ„ ì½”ë“œì²˜ëŸ¼ ë°”ê¾¸ì—ˆì„ ë•Œ ë‹¤ìŒê³¼ ê°™ë‹¤.
        # q, k, v = [l(x) for l, x in zip(linears, (q, k, v))]
        
        q = q.view(query.size(0), -1, n_heads, head_dim)
        k = q.view(query.size(0), -1, n_heads, head_dim)
        v = q.view(query.size(0), -1, n_heads, head_dim)
        # ì—¬ê¸°ê¹Œì§€ë¥¼ ìœ„ ì½”ë“œì²˜ëŸ¼ ë°”ê¾¸ì—ˆì„ ë•Œ ë‹¤ìŒê³¼ ê°™ë‹¤.
        # q, k, v = [l(x).view(query.size(0), -1, n_heads, head_dim) for l, x in zip(linears, (q, k, v))]
        
        q = q.transpose(1, 2)
        k = q.transpose(1, 2)
        v = q.transpose(1, 2)
        # ì—¬ê¸°ê¹Œì§€ ìœ„ ì½”ë“œì²˜ëŸ¼ ë°”ê¾¸ì—ˆì„ ë•Œ ë‹¤ìŒê³¼ ê°™ë‹¤.
        # q, k, v = [l(x).view(bs, -1, n_heads, head_dim).transpose(1, 2) for l, x in zip(linears, (q, k, v))]
        '''
        nbatches = query.size(0)
        
        #Scaled Dot-Product Attention ì„ ìˆ˜í–‰
        x = self.attn(query, key, value, mask=mask)
        
 
        #ë¶„ë¦¬ëœ headë“¤ì„ concat í•˜ê¸° -> (batch_size, seq_len, d_model)
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        #.transposeëŠ” ê¸°ë³¸ ë©”ëª¨ë¦¬ ì €ì¥ì†Œë¥¼ ì›ë˜ì˜ tensorì™€ ê³µìœ í•˜ê¸° ë•Œë¬¸ì— .contiguous ë°©ë²•ì€ .transpose ë‹¤ìŒì— ì¶”ê°€
        #.viewë¥¼ í˜¸ì¶œí•˜ë ¤ë©´ ì¸ì ‘í•œ(contiguous) tensorê°€ í•„ìš”
        return self.final_linear(x)

#%%
#ë ˆì§€ë“€ì–¼ ë° ë ˆì´ì–´ ì •ê·œí™”(Residuals and Layer Normalization)
#ë ˆì´ì–´ ì •ê·œí™”
from torch import nn
class LayerNorm(nn.Module):
    def __init__(self, features: int, eps: float = 1e-6):
        super(LayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(features))
        self.beta = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    
#ë ˆì§€ë“€ì–¼
#ë„¤íŠ¸ì›Œí¬ì—ì„œ ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ì„ (ì¦‰, í•˜ìœ„ ë ˆì´ì–´) í˜„ì¬ ë ˆì´ì–´ì˜ ì¶œë ¥ì— ì¶”ê°€ ->very deep network
#residualconnection(x)=x+Dropout(SubLayer(LayerNorm(x)))
from torch import nn
class ResidualConnection(nn.Module):
    def __init__(self, size: int = 512, dropout: float = .1):
        super(ResidualConnection,  self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x))) 

#%%
#í”¼ë“œ í¬ì›Œë“œ(Feed Forward)
# ReLUReLUReLU í™œì„±í™”(activation) (ReLU(x)=max(0,x)ReLU(x) = max(0, x)ReLU(x)=max(0,x))) ì™€ ë‚´ë¶€ ë ˆì´ì–´ ë“œë¡­ì•„ì›ƒì„ í†µí•´ ì™„ì „íˆ ì—°ê²°ëœ (fully-connected) ë ˆì´ì–´ë¡œ êµ¬ì„±
#FeedForward(x) = W2max(0,xW1+B1)+B2
from torch import nn
class FeedForward(nn.Module):
    def __init__(self, d_model: int = 512, d_ff: int = 2048, dropout: float = .1):
        super(FeedForward, self).__init__()
        self.l1 = nn.Linear(d_model, d_ff)
        self.l2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.l2(self.dropout(self.relu(self.l1(x))))
 
#%%   
#ì¸ì½”ë”-ë””ì½”ë”
#ì¸ì½”ë”
#Encoding(x,mask)=FeedForward(MultiHeadAttention(x))
from torch import nn
from copy import deepcopy
class EncoderLayer(nn.Module):
    def __init__(self, size: int, self_attn: MultiHeadAttention, feed_forward: FeedForward, dropout: float = .1):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sub1 = ResidualConnection(size, dropout)
        self.sub2 = ResidualConnection(size, dropout)
        self.size = size

    def forward(self, x, mask):
        # Multi-Head Attention -> residual connection
        x = self.sub1(x, lambda x: self.self_attn(x, x, x, mask))
        # Multi Head Attention ì—ì„œ Query, Key, Value ìë¦¬ì— ë™ì¼í•œ x ->ë¬¸ì¥ ë‚´ì˜ í† í°ë¼ë¦¬ ê´€ê³„ë„ë¥¼ ì—°ì‚° ëª©ì  
        return self.sub2(x, self.feed_forward)
    '''
    # Input x: [batch_size, seq_len, hid_dim] 
    # Output: ë™ì¼í•œ Shape
    '''
#ì¸ì½”ë”ëŠ” ë ˆì´ì–´ ì •ê·œí™”ê°€ ë’¤ë”°ë¥´ëŠ” 6ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´ ì •ê·œí™”ë¡œ êµ¬ì„±
class Encoder(nn.Module):
    def __init__(self, layer, n: int = 6):
        super(Encoder, self).__init__()
        self.layers = nn.ModuleList([deepcopy(layer) for _ in range(n)])
        '''
        xê°€ layer ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê³  ê·¸ì— ëŒ€í•œ ê²°ê³¼ë¬¼ì„ xë¡œ ë°›ëŠ”ë‹¤.
        for ë¬¸ ì•ˆì—ì„œ ì´ xê°€ ë‹¤ì‹œ layerì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ëœë‹¤.
        ì´ëŸ° ë°©ë²•ìœ¼ë¡œ EncoderLayer 6ê°œ ìƒì„±
        '''
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
    # output x: [batch_size, seq_len, hid_dim]
#%%
#ë””ì½”ë”
'''
ë©”ëª¨ë¦¬ë¥¼ í¬í•¨í•œ ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜ ë ˆì´ì–´ê°€ ë’¤ë”°ë¥´ëŠ” ë§ˆìŠ¤í‚¹ëœ (maksed) ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜ ë ˆì´ì–´
-> ë©”ëª¨ë¦¬ëŠ” ì¸ì½”ë”ì˜ ì¶œë ¥
'''
#Decoding(x, memory, mask1, mask2) = FeedForward(MultiHeadAttention(MultiHeadAttention(x,mask1),,memory,mask2))
from torch import nn
from copy import deepcopy
class DecoderLayer(nn.Module):
    def __init__(self, size: int, self_attn: MultiHeadAttention, src_attn: MultiHeadAttention, 
                 feed_forward: FeedForward, dropout: float = .1):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sub1 = ResidualConnection(size, dropout)
        self.sub2 = ResidualConnection(size, dropout)
        self.sub3 = ResidualConnection(size, dropout)
 
    def forward(self, x, memory, src_mask, tgt_mask):
        #1. masked multi-head attention (self attention) + add
        x = self.sub1(x, lambda x: self.self_attn(x, x, x, tgt_mask))
        
        #2. multi-head attention (encoder-decoder attention)
        x = self.sub2(x, lambda x: self.src_attn(x, memory, memory, src_mask))
        #queryëŠ” decoderì—ì„œ ì˜¬ë¼ì˜¤ê³ , key, valueëŠ” encoderì—ì„œ ë„˜ì–´ì˜¨ ê°’(memory)ì„ ì‚¬ìš©
        
        return self.sub3(x, self.feed_forward)

#ë””ì½”ë”ëŠ” ë ˆì´ì–´ ì •ê·œí™”ê°€ ë’¤ë”°ë¥´ëŠ” 6ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´ë¥¼ ê°€ì§€ê³  ìˆë‹¤
class Decoder(nn.Module):
    def __init__(self, layer: DecoderLayer, n: int = 6):
        super(Decoder, self).__init__()
        self.layers = nn.ModuleList([deepcopy(layer) for _ in range(n)])
        self.norm = LayerNorm(layer.size)

    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)


class Output(nn.Module):
    def __init__(self, input_dim: int, output_dim: int):
        super(Output, self).__init__()
        self.l1 = nn.Linear(input_dim, output_dim)
        self.log_softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        logits = self.l1(x)
        return self.log_softmax(logits)


class EncoderDecoder(nn.Module):
    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: Embed, tgt_embed: Embed, final_layer: Output):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.final_layer = final_layer
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        return self.final_layer(self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask))
    
    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)
    
    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)

#%%
#ìµœì¢… ì¶œë ¥
#ë””ì½”ë”ì˜ ë²¡í„° ì¶œë ¥ì„ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ë³€í™˜
#ë””ì½”ë” ì¶œë ¥->logitsì˜ í–‰ë ¬ë¡œ, ì´ëŠ” íƒ€ê¹ƒ ì–´íœ˜ì˜ ì°¨ì›ì„ ê°–ê³  ìˆìŒ
#->softmax í™œì„±í™” í•¨ìˆ˜ë¥¼ í†µí•´ ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
#Output(x) = LogSoftmax(max(0,xW1+B1))

#%%    
#ëª¨ë¸ ì´ˆê¸°í™”
from torch import nn
def make_model(input_vocab: int, output_vocab: int, d_model: int = 512):
    encoder = Encoder(EncoderLayer(d_model, MultiHeadAttention(), FeedForward()))
    decoder = Decoder(DecoderLayer(d_model, MultiHeadAttention(), MultiHeadAttention(), FeedForward()))
    input_embed= nn.Sequential(Embed(vocab=input_vocab), PositionalEncoding())
    output_embed = nn.Sequential(Embed(vocab=output_vocab), PositionalEncoding())
    output = Output(input_dim=d_model, output_dim=output_vocab)
    model = EncoderDecoder(encoder, decoder, input_embed, output_embed, output)
    
    # Initialize parameters with Xavier uniform 
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    return model

#%%
# Tokenized symbols for source and target.
src = torch.tensor([[1, 2, 3, 4, 5]])
src_mask = torch.tensor([[1, 1, 1, 1, 1]])
tgt = torch.tensor([[6, 7, 8, 0, 0]])
tgt_mask = torch.tensor([[1, 1, 1, 0, 0]])

# Create PyTorch model
model = make_model(input_vocab=10, output_vocab=10)
#ì…ë ¥ê³¼ ì¶œë ¥ì— ëŒ€í•œ ì–´íœ˜ê°€ 10ê°œë§Œ ìˆë‹¤ê³  ê°€ì •

# Do inference and take tokens with highest probability through argmax along the vocabulary axis (-1)
result = model(src, tgt, src_mask, tgt_mask)
result.argmax(dim=-1) #tensor([[4, 4, 3, 4, 4]])
